{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBWmiHimikNZPDNvBhxAoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueop/Classification-Practice/blob/main/Classification_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0lZ_mHRKT_m"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# drive.mount('/content/drive')   #구글 드라이브 연결\n",
        "# Project_Path = '/content/drive/MyDrive/AS_LAB'  #저장할 폴더 경로 설정\n",
        "\n",
        "# #저장할 폴더 존재 여부 확인(없으면 새 폴더 생성, 있으면 기존 폴더 사용)\n",
        "# if not os.path.exists(Project_Path):\n",
        "#     os.makedirs(Project_Path)\n",
        "#     print(f\"새 폴더 생성 후 저장 완료: {Project_Path}\")\n",
        "# else:\n",
        "#     print(f\"기존 폴더에 저장: {Project_Path}\")\n",
        "\n",
        "# %cd $Project_Path   #작업 위치 이동(change directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 코드는 Ben Trevett의 튜토리얼을 참고하여 학습 목적으로 작성되었습니다.\n",
        "# Source: https://github.com/bentrevett/pytorch-image-classification\n",
        "\n",
        "#Data Precessing\n",
        "import torch #일반적인 파이토치 기능(텐서 만들기 등)\n",
        "import torch.nn as nn #신경망 층 쌓을 때 사용(nn.Linear, nn.CrossEntropyLoss 등)\n",
        "import torch.nn.functional as F #nn의 함수 형태 기능들 사용\n",
        "import torch.optim as optim #최적화(Optimizer) 도구들\n",
        "import torch.utils.data as data #데이터를 쉽게 퍼나르기 위한 도구(DataLoader를 통해 데이터를 한 번에 몇 개씩(Batch)가져올지, 섞어서 (Shuffle)가져올지 결정)\n",
        "\n",
        "import torchvision.transforms as transforms #이미지를 변형하는 도구\n",
        "import torchvision.datasets as datasets #데이터셋을 모아놓은 라이브러리(datasets.MNIST 등)\n",
        "\n",
        "from sklearn import metrics #정확도나 혼동행렬 계산시 사용(채점표)\n",
        "from sklearn import decomposition #어려운 고차원 데이터를 2차원 평면으로 압축하여 시각화할 때 사용(PCA)\n",
        "from sklearn import manifold #어려운 고차원 데이터를 2차원 평면으로 압축하여 시각화할 때 사용(t-SNE)\n",
        "from tqdm.notebook import trange, tqdm #진행률 표시바(Progress bar)\n",
        "import matplotlib.pyplot as plt #시각화 도구\n",
        "import numpy as np #수치 계산시 사용\n",
        "\n",
        "import copy #학습 중 성능이 가장 좋았던 모델의 가중치를 복사해서 저장해둘 때 사용\n",
        "import random #실험결과를 똑같이 재현하기 위해 랜덤 시드를 고정할 때 사용\n",
        "import time #학습시간 측정시 사용\n"
      ],
      "metadata": {
        "id": "xU4gyks5MO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#재현할 수 있는 결과를 얻게하기 위해 랜덤 시드 설정\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "0qzgnBXOwBX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 로드하기\n",
        "ROOT = '.data' #데이터가 저장될 위치 지정(폴더명)\n",
        "train_data = datasets.MNIST(root=ROOT, #MNIST 데이터셋을 ROOT 위치에 저장\n",
        "                            train=True, #학습용 데이터 로드(6만장)\n",
        "                            download=True) #ROOT 위치에 데이터셋이 이미 있다면 다운로드 하지 않고, 없으면 다운로드"
      ],
      "metadata": {
        "id": "U7EJU9taxrwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평균과 표준편차 구하기\n",
        "mean = train_data.data.float().mean()/255\n",
        "std = train_data.data.float().std()/255"
      ],
      "metadata": {
        "id": "SKv7vne125Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('평균: ', mean)\n",
        "print('표준편차: ', std)"
      ],
      "metadata": {
        "id": "38IZvHcT3Vgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 증강(Rotation, Crop, ToTensor, Normalize)\n",
        "train_transforms = transforms.Compose([ #훈련용 데이터\n",
        "    transforms.RandomRotation(5, fill=(0,)), #이미지를 시계/반시계 방향으로 -5도에서 +5도 사이로 랜덤하게 회전 / fill=(0,): 이미지 회전시 귀퉁이에 생기는 빈 공간 0으로 채우기\n",
        "    transforms.RandomCrop(28, padding=2), #28x28 이미지 테두리에 2픽셀씩 덧붙여 32x32크기로 키운 후 28x28 크기로 무작위 추출(이미지가 상하좌우로 이동하는 효과)\n",
        "    transforms.ToTensor(), #그림 파일을 Tensor로 형변환(0~255의 값이 0~1 사이의 실수로 변형)\n",
        "    transforms.Normalize(mean=[mean], std=[std]) #데이터에서 평균을 빼고, 표준편차로 나누기(데이터의 분포를 0 중심으로 모아 학습 효율 극대화)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([ #실험용 데이터\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean], std=[std])\n",
        "])"
      ],
      "metadata": {
        "id": "p9ixkFcu4FEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 셋을 훈련용, 실험용으로 나누어 로드하고, 각각 데이터셋에 맞는 변형 선택\n",
        "train_data = datasets.MNIST(root=ROOT,\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=train_transforms)\n",
        "\n",
        "test_data = datasets.MNIST(root=ROOT,\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=test_transforms)"
      ],
      "metadata": {
        "id": "DYFmJMAZ-Rmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 개수 확인\n",
        "print(f'훈련용 데이터 예제 수: {len(train_data)}')\n",
        "print(f'실험용 데이터 예제 수: {len(test_data)}')"
      ],
      "metadata": {
        "id": "HNZDnhin_DX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 시각화\n",
        "def plot_images(images):\n",
        "\n",
        "    n_images = len(images) #이미지 개수\n",
        "\n",
        "    rows = int(np.sqrt(n_images)) #이미지 개수의 제곱근\n",
        "    cols = int(np.sqrt(n_images)) #이미지 개수의 제곱근\n",
        "\n",
        "    fig = plt.figure() #그림이 그려질 곳(도화지)\n",
        "    for i in range(rows*cols): #이미지 수만큼 반복\n",
        "        ax = fig.add_subplot(rows, cols, i+1) #작은 그래프 하나 추가 / i+1: 그림 순서를 1번부터 세기 때문에 지정\n",
        "        ax.imshow(images[i].view(28,28).cpu().numpy(), cmap='bone') #i번째 이미지 데이터를 꺼내 28x28크기의 이미지로 돌려놓고, 텐서를 CPU로 가져와서 넘파이 배열로 바꾸는 명령\n",
        "        ax.axis('off') #그래프의 눈금 지우기"
      ],
      "metadata": {
        "id": "4AtLfKr8B8Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#함수 테스트\n",
        "N_IMAGES = 25\n",
        "images = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\n",
        "plot_images(images)"
      ],
      "metadata": {
        "id": "h8lW8gBNF-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Set(검증용 데이터셋) 만들기\n",
        "VALID_RATIO = 0.9\n",
        "\n",
        "n_train_examples = int(len(train_data)*VALID_RATIO)\n",
        "n_valid_examples = len(train_data) - n_train_examples #train_dataset의 10%를 Validation Set으로 사용"
      ],
      "metadata": {
        "id": "Z1V9hoXJLEuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#검증용 데이터 랜덤 분할\n",
        "train_data, valid_data = data.random_split(train_data,\n",
        "                                           [n_train_examples, n_valid_examples])"
      ],
      "metadata": {
        "id": "Ng5TaqY0Mpl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#분할 수 확인\n",
        "print(f'훈련용 데이터 개수: {len(train_data)}')\n",
        "print(f'검증용 데이터 개수: {len(valid_data)}')\n",
        "print(f'실험용 데이터 개수: {len(test_data)}')"
      ],
      "metadata": {
        "id": "i8VeP2jUNlKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#검증용 데이터셋 확인\n",
        "N_IMAGES = 25\n",
        "\n",
        "images = [image for image, label in [valid_data[i] for i in range(N_IMAGES)]]\n",
        "\n",
        "plot_images(images)"
      ],
      "metadata": {
        "id": "ma_hRTsBONHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#얕은 복사 문제와 검증용 데이터셋의 변형 상태를 돌려놓기 위한 설정\n",
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ],
      "metadata": {
        "id": "aeIdtHaNPCoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#설정 확인\n",
        "N_IMAGES = 25\n",
        "\n",
        "images = [image for image, label in [valid_data[i] for i in range(N_IMAGES)]]\n",
        "\n",
        "plot_images(images)"
      ],
      "metadata": {
        "id": "nBvQz5AXPpNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로더 만들기\n",
        "BATCH_SIZE = 64 #한 번에 64개의 데이터를 모델에 입력하여 처리\n",
        "\n",
        "train_iterator = data.DataLoader(train_data,\n",
        "                                 shuffle=True, #매번 새로운 조합의 데이터 묶음을 보여준다.\n",
        "                                 batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_iterator = data.DataLoader(valid_data,\n",
        "                                  batch_size=BATCH_SIZE)\n",
        "\n",
        "test_iterator = data.DataLoader(test_data,\n",
        "                                batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "6e5mMUF3P1UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle=True: 훈련시에는 이미지의 순서를 보고 값을 예측하지 못하도록 매번 새로운 조합의 데이터 묶음을 보여주어 제대로 내용을 이해하도록 해준다(확률적 경사 하강법(SGD)의 핵심)."
      ],
      "metadata": {
        "id": "xQn-wgaeUiSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Model\n",
        "#MLP 클래스 구현\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    #생성자\n",
        "    def __init__(self, input_dim, output_dim): #모델이 사용할 층들을 정의하는 함수\n",
        "        super().__init__() #자식 클래스가 부모 클래스의 초기화 메소드를 호출하여 부모의 속성과 기능을 그대로 가져와 사용하는 코드\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 250) #1층: 입력 -> 은닉층1(입력 데이터를 250차원의 특징 벡터로 변환(매핑))\n",
        "        self.hidden_fc = nn.Linear(250, 100) #2층: 은닉층1 -> 은닉층2(250차원의 특징을 더 압축하여 100차원의 고급특징 벡터로 매핑)\n",
        "        self.output_fc = nn.Linear(100, output_dim) #3층: 은닉층2 -> 출력(100차원의 특징 벡터를 보고 0~9까지의 클래스 확률로 변환\n",
        "\n",
        "    #순전파 함수\n",
        "    def forward(self, x): #데이터(x)가 실제로 들어왔을 때 모델을 어떤 순서로 통과할지 결정\n",
        "\n",
        "        #현재 x = [batch size, height, width]\n",
        "\n",
        "        #평탄화\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        #현재 x = [batch size, height*width]\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x)) #784차원의 입력 벡터를 250차원의 특징 벡터로 변환(숫자의 기초적인 특징, ReLU를 이용해 불필요한 정보는 삭제)\n",
        "\n",
        "        #은닉층1 h_1 = [batch size, 250]\n",
        "\n",
        "        h_2 = F.relu(self.hidden_fc(h_1)) #250차원의 특징 벡터를 100차원의 특징 벡터로 압축(기초 특징들을 조합해 고차원적인 정보를 담은 특징 벡터로 압축)\n",
        "\n",
        "        #은닉층2 h_2 = [batch size, 100]\n",
        "\n",
        "        y_pred = self.output_fc(h_2) #100차원 특징 정보를 분석해 10개의 클래스 점수 계산\n",
        "\n",
        "        #y_pred = [batch size, output dim]\n",
        "\n",
        "        return y_pred, h_2 #y_pred: 최종 판결(정답), h_2: 판결을 내리기 위해 사용한 100차원 특징 벡터(시각화하여 모델 분석에 사용)"
      ],
      "metadata": {
        "id": "RnqWpBd9U5MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x.view(batch_size, -1): view 함수를 이용하여 차원 재구성(28x28 이미지를 1x784의 평면으로 변환)한다.\n",
        "batch_size, -1 파라미터: x의 텐서 안에는 64(batch size), 1(channel), 28(width), 28(height)개를 곱한 50,176개의 데이터가 들어있다. view(batch_size, -1)함수가 작동하면 첫 번째 차원을 batch_size(64)로 고정하고, 다른 차원은 컴퓨터가 알아서 채우게 된다(50,176 / 64 = 784). 따라서 텐서의 모양은[64, 784]로 변하게 되고 이 것이 평탄화 작업이다."
      ],
      "metadata": {
        "id": "_AV2w5qp9iNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 정의 및 입출력차원 설정\n",
        "INPUT_DIM = 28 * 28\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "model = MLP(INPUT_DIM, OUTPUT_DIM)"
      ],
      "metadata": {
        "id": "mKGfG2QoCb0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델로 학습 가능한 파라미터(가중치, 편향)의 총 개수를 계산하는 함수 정의\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) #학습 가능한(미분해야되는) 파라미터의 수 리턴"
      ],
      "metadata": {
        "id": "v6cVlPlIGGZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'이 모델은 {count_parameters(model):,}개의 학습 가능한 파라미터를 가집니다.')"
      ],
      "metadata": {
        "id": "sxVK0GiWHzum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training The Model\n",
        "\n",
        "#optimizer 정의\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "f_9LCfS6KayB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer: 최적화 도구, 즉 손실 함수(오차)를 최소화하는 최적의 모델 매개변수를 찾는 알고리즘"
      ],
      "metadata": {
        "id": "5Jv8r0UdN1yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#손실 함수 정의\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "kRyB5xFxOGjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device 정의\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0A5QgRjsUhFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device에서 실행되도록 변경\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "qsUh0sjdW3PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정확도 측정 함수\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True) #예측값 중 가장 가장 높은 확률을 가진 번호 찾기\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum() #정답과 비교\n",
        "    acc = correct.float() / y.shape[0] #정답의 수를 입력 데이터 개수로 나누어 정확도 계산\n",
        "    return acc"
      ],
      "metadata": {
        "id": "sTZlWFdqW28c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "arg: 인자, max: 최대값, 1: 가로방향(클래스 점수)에서 찾기, keepdim=True: 원래 모양(2차원) 유지.\n",
        "\n",
        "y.view_as(top_pred): 일반적으로 1차원 가로줄인 y(정답)를 2차원 세로줄인 top_pred(예측)과 같은 모양으로 바꾸는 명령(컴퓨터는 모양이 다르면 비교 불가), eq: 비교(같으면 True, 틀리면 False), sum(): 맞춘 개수 다 더하기.\n",
        "\n",
        "float(): 소수점까지 구하기 위해 실수로 변경, / y.shape[0]: 입력 데이터 개수(batch size)로 나누기."
      ],
      "metadata": {
        "id": "17ODj0B5ZcdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 실행 함수\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train() #모델을 학습 모드로 전환\n",
        "\n",
        "    #반복학습 시작\n",
        "    for(x, y) in tqdm(iterator, desc=\"Training\", leave=False): #iterator: DataLoader가 64개씩(batch size) 묶어서 주는 문제(x), 정답(y) 묶음, tqdm: 진행율 표시\n",
        "        x = x.to(device) #.to(device): 데이터를 CPU에서 GPU로 옮김\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad() #이전에 계산했던 기울기(gradient) 기록 지우기\n",
        "\n",
        "        y_pred, _ = model(x) #예측(forward), _: 모델이 반환한 y_pred, h_2 값 중 h_2는 학습시 버린다는 의미\n",
        "\n",
        "        loss = criterion(y_pred, y) #채점(loss 계산)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y) #정확도 계산\n",
        "\n",
        "        loss.backward() #역전파(틀린부분 을 찾기위해 역추적하며 각 뉴런이 수정해야할 방향(기울기) 계산)\n",
        "\n",
        "        optimizer.step() #가중치 갱신(계산된 방향으로 가중치(파라미터)를 조금씩 수정)\n",
        "\n",
        "        #이번 배치(64개)에서 얻은 점수와 정확도 누적\n",
        "        epoch_loss += loss.item() #.item: 텐서(Tensor) 형태의 값을 실수(float)형으로 꺼내오기\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator) #평균 손실과 평균 정확도 반환"
      ],
      "metadata": {
        "id": "hHaTptFDU1Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가 진행 함수\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval() #모델을 평가 모드로 전환\n",
        "\n",
        "    with torch.no_grad(): #기울기 기록 X\n",
        "        for (x,y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
        "\n",
        "            x = x.to(device) #데이터를 CPU에서 GPU로 이동\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred, _ = model(x) #예측\n",
        "\n",
        "            loss = criterion(y_pred, y) #손실 계산(예측값(y_pred)과 정답(y)의 오차 계산)\n",
        "\n",
        "            acc = calculate_accuracy(y_pred, y) #정확도 계산\n",
        "\n",
        "            epoch_loss += loss.item() #텐서 형태의 값 실수 형태로 가져오기\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator) #평균 손실 및 평균 정확도 반환"
      ],
      "metadata": {
        "id": "jgwlJJV3HeFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#현재 데이터 구간 시간 계산 함수 정의\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time #실행시간(초) 계산\n",
        "    elapsed_mins = int(elapsed_time / 60) #실행시간(분) 계산\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) #남은시간(초) 계산\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "8i3cFBtLPYLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 실행 및 결과 출력\n",
        "EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid ACC: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "aluDcZfuQcQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#성능 평가를 위해 가장 손실이 적은 파라미터 저장\n",
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)"
      ],
      "metadata": {
        "id": "6VUHgT0heF7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#성능 출력\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "72-6Dny8ejKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 검토\n",
        "\n",
        "#모델의 예측 보는 함수\n",
        "def get_predictions(model, iterator, device):\n",
        "\n",
        "    model.eval() #평가 모드로 변경\n",
        "\n",
        "    images = [] #리스트 생성\n",
        "    labels = []\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (x,y) in iterator:\n",
        "\n",
        "            x = x.to(device) #데이터를 device로 이동\n",
        "\n",
        "            y_pred, _ = model(x) #예측\n",
        "\n",
        "            y_prob = F.softmax(y_pred, dim=-1)\n",
        "\n",
        "            images.append(x.cpu()) #cpu로 가져와서 리스트에 저장\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(y_prob.cpu())\n",
        "\n",
        "    #조각난 배치 하나로 이어붙이기\n",
        "    images = torch.cat(images, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    probs = torch.cat(probs, dim=0)\n",
        "    return images, labels, probs"
      ],
      "metadata": {
        "id": "xp_uDIGBenFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#예측값 얻기\n",
        "images, labels, probs = get_predictions(model, test_iterator, device)\n",
        "\n",
        "pred_labels = torch.argmax(probs, 1)"
      ],
      "metadata": {
        "id": "vFaFUCemhvlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#혼동 행렬 만들기\n",
        "def plot_confusion_matrix(labels, pred_labels):\n",
        "    fig = plt.figure(figsize=(10,10)) #그림판 생성\n",
        "    ax = fig.add_subplot(1,1,1) #서브플롯 추가\n",
        "    cm = metrics.confusion_matrix(labels, pred_labels) #혼동 행렬 값 계산\n",
        "    cm = metrics.ConfusionMatrixDisplay(cm, display_labels=range(10)) #혼동 행렬 시각화 객체 생성\n",
        "    cm.plot(values_format='d', cmap='Blues', ax=ax) #그래프 그리기"
      ],
      "metadata": {
        "id": "xxzJkMotjBir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#그래프 출력\n",
        "plot_confusion_matrix(labels, pred_labels)"
      ],
      "metadata": {
        "id": "oqoBCIN_kxHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#예측 정답 여부 확인\n",
        "corrects = torch.eq(labels, pred_labels)"
      ],
      "metadata": {
        "id": "yQkejc3ZlH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#틀린 데이터 확인\n",
        "incorrect_examples = []\n",
        "\n",
        "for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
        "    if not correct:\n",
        "        incorrect_examples.append((image, label, prob))\n",
        "\n",
        "incorrect_examples.sort(reverse=True, key=lambda x: torch.max(x[2], dim=0).values)"
      ],
      "metadata": {
        "id": "oXb8dxk4mEWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#틀린 데이터 시각화\n",
        "def plot_most_incorrect(incorrect, n_images):\n",
        "\n",
        "    #이미지의 크기만큼 그림판 생성\n",
        "    rows = int(np.sqrt(n_images))\n",
        "    cols = int(np.sqrt(n_images))\n",
        "\n",
        "    fig = plt.figure(figsize=(20,10)) #그림판 생성\n",
        "    for i in range(rows*cols): #이미지 크기만큼 반복\n",
        "        ax = fig.add_subplot(rows, cols, i+1) #서브플롯 생성\n",
        "        image, true_label, probs = incorrect[i]\n",
        "        true_prob = probs[true_label]\n",
        "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
        "        ax.imshow(image.view(28, 28).cpu().numpy(), cmap='bone')\n",
        "        ax.set_title(f'true label: {true_label} ({true_prob:.3f}\\n'\n",
        "                     f'pred label: {incorrect_label} ({incorrect_prob:.3f})')\n",
        "        ax.axis('off') #그래프 눈금 지우기\n",
        "    fig.subplots_adjust(hspace=0.5)"
      ],
      "metadata": {
        "id": "Ho_d3rq_ohsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#확인\n",
        "N_IMAGES = 25\n",
        "\n",
        "plot_most_incorrect(incorrect_examples, N_IMAGES)"
      ],
      "metadata": {
        "id": "0pR0VQ6es_KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델의 생각과정 출력\n",
        "def get_representations(model, iterator, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    outputs = []\n",
        "    intermediates = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (x, y) in tqdm(iterator):\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            y_pred, h = model(x)\n",
        "\n",
        "            outputs.append(y_pred.cpu())\n",
        "            intermediates.append(h.cpu())\n",
        "            labels.append(y.cpu())\n",
        "\n",
        "    outputs = torch.cat(outputs, dim=0)\n",
        "    intermediates = torch.cat(intermediates, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    return outputs, intermediates, labels"
      ],
      "metadata": {
        "id": "bh8ODSOwtfQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#출력\n",
        "outputs, intermediates, labels = get_representations(model, test_iterator, device)"
      ],
      "metadata": {
        "id": "TsvHNEVwvWfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#주성분 분석(PCA) 함수\n",
        "def get_pca(data, n_components=2):\n",
        "    pca = decomposition.PCA() #사이킷런 라이브러리에서 PCA 꺼냄\n",
        "    pca.n_components = n_components #데이터를 몇차원으로 줄일지 결정\n",
        "    pca_data = pca.fit_transform(data) #데이터의 특징 파악 후\n",
        "    return pca_data"
      ],
      "metadata": {
        "id": "fICNNaP8wN5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA 역할\n",
        "- 차원 축소: 데이터 셋의 변수가 너무 많을 때 데이터가 가진 핵심 정보를 최대한 유지하면서 변수의 수 줄인다. 이는 데이터 분석 및 처리 속도를 향상시킨다.\n",
        "- 데이터 전처리: 복잡한 데이터를 단순화하여 모델의 과적합 방지.\n",
        "\n",
        "작동 원리\n",
        "1. 새로운 축 설정: 데이터의 분산이 가장 큰 방향(정보량이 많은 방향)을 첫 번째 주성분으로 지정\n",
        "2. 직교성: 그 다음 주성분들은 이전 주성분들과 상관관계가 없도록 설정한다.\n",
        "3. 정보 보존: 분산이 큰 주성분들만 선택적으로 사용하여 원본 데이터의 가장 중요한 정보를 보존하면서 차원을 축소한다.\n"
      ],
      "metadata": {
        "id": "OiUR-C1D4AmF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s5TS9YS6lhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}